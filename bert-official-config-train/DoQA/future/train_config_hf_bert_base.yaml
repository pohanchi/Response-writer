bottleneck_size: 16
add_cross_attention: False
exp_name: DoQA_hf_bert_future
epoches: 20
weight_decay: 0.0
learning_rate: 0.00003
warmup_steps: 1000
adam_epsilon: 1.0e-6
adam_beta1: 0.9
adam_beta2: 0.999
batch_size: 6
gradient_accumulation_steps: 2
local_rank: -1
seed: 42
num_labels: 2
dialog_labels: 2
fp16: False
max_grad_norm: 10.0
logging_steps: 200
save_steps: 200
output_dir: ../CQAModel/DoQA/Bert_HF_future/
evaluate_during_training: True
version_2_with_negative: True
verbose_logging: True
max_answer_length: 30
n_best_size: 20
null_score_diff_threshold: 0.0
do_lower_case: True
train_feature_file: ../preprocessing_files/bert/DoQA/future/base_train_file_cooking_HAE
eval_feature_file: ../preprocessing_files/bert/DoQA/future/base_dev_file_cooking_HAE
eval_json: ../dataset_local/doqa/downloads/extracted/fbd54edd7b6dab377ff9e01e3d49da6799e7eaca633f57cecf57f0748ee3688f/doqa-v2.1/doqa_dataset/doqa-cooking-dev-v2.1.json

model: BERTQA_memory23_future
pretrained_name: bert-base-uncased

model_config:
  pos_att_type: "c2p|p2c"
  relative_attention: False
  max_relative_positions: 10
